{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     BitsAndBytesConfig,\n",
    "# )\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model_name):\n",
    "        pass\n",
    "\n",
    "    def make_request(self, prompt):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OpenAIClient(LLMClient):\n",
    "    def __init__(self, model_name):\n",
    "        self.client = OpenAI()\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def make_request(self, prompt):\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.001,\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "class OllamaClient(LLMClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"llama3.2:1b\",\n",
    "        logger=None,\n",
    "        is_think_model=False,\n",
    "        temperature=0.001,\n",
    "        max_new_tokens=1000,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.client = OllamaLLM(model=model_name, temperature=0.001)\n",
    "        self.logger = logger\n",
    "        self.is_think_model = is_think_model\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def make_request(self, prompt):\n",
    "        response = self.client.invoke(\n",
    "            prompt,\n",
    "        )\n",
    "        if \"</think>\" in response:\n",
    "            response = response.split(\"</think>\")[1]\n",
    "            response = response.split(\"<think>\")[0]\n",
    "\n",
    "        response = response.strip().replace(\"\\n\", \"\")\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# class HuggingFaceClient(LLMClient):\n",
    "#     def __init__(\n",
    "#         self, model_name=\"meta-llama/Llama-3.2-3B\", mode=\"auto\", max_new_tokens=300\n",
    "#     ):\n",
    "\n",
    "#         bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16\n",
    "#         )\n",
    "\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         print(device)\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(\n",
    "#             pretrained_model_name_or_path=model_name,\n",
    "#             return_dict=True,\n",
    "#             quantization_config=bnb_config,\n",
    "#             attn_implementation=\"flash_attention_2\",\n",
    "#         )\n",
    "#         if self.tokenizer.pad_token is None:\n",
    "#             self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "#             self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "#         self.device = device\n",
    "#         self.model.to(device)\n",
    "#         self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "#         self.max_new_tokens = max_new_tokens\n",
    "\n",
    "#     def make_request(self, prompt):\n",
    "#         final_input = self.tokenizer(\n",
    "#             prompt,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#         ).to(self.device)\n",
    "\n",
    "#         raw_outputs = self.model.generate(\n",
    "#             **final_input, max_new_tokens=self.max_new_tokens\n",
    "#         )\n",
    "#         decoded_outputs = self.tokenizer.batch_decode(\n",
    "#             raw_outputs, skip_special_tokens=True\n",
    "#         )\n",
    "\n",
    "#         only_generated_ouput = [\n",
    "#             ind_decoded_output.replace(ind_prompt, \"\")\n",
    "#             for ind_prompt, ind_decoded_output in zip(prompt, decoded_outputs)\n",
    "#         ]\n",
    "#         return only_generated_ouput\n",
    "\n",
    "\n",
    "# SIMPLE_PROMPT_COLUMN = \"\"\"\n",
    "# You are supplied with the content of a specific column from a database and its current name.\n",
    "# This name is not representative, meaning it does not accurately describe the content of the column.\n",
    "# You are tasked with rephrasing the name of the column to better reflect its content.\n",
    "# Remember that this name should be simple and also descriptive.\n",
    "# The current column name is: \"{column_name}\"\n",
    "# The content of the column is as follows: \"{content}\"\n",
    "# Your response should come in the following format:\n",
    "# {{\"rephrased_column_name\": \"new_column_name\"}}\n",
    "# The new name must be a contiguous string. No spaces or special characters in it.\n",
    "# And only that, nothing more is accepted.\n",
    "# Only generate one new name per column.\n",
    "# It is obligatory to respond with a JSON object. And only that.\n",
    "# Respect the JSON format.\n",
    "# A JSON has only one {{ in the beginning and one }} in the end.\n",
    "# \"\"\"\n",
    "# SIMPLE_PROMPT_TABLE = \"\"\"\n",
    "# You are supplied with the content of a specific table from a database and its current name.\n",
    "# This name is not representative, meaning it does not accurately describe the content of the table.\n",
    "# You are tasked with rephrasing the name of the table to better reflect its content.\n",
    "# Remember that this name should be simple and also descriptive.\n",
    "# The current table name is: \"{table_name}\"\n",
    "# The content of the talbe is as follows: \"{content}\"\n",
    "# Your reponse should come in the following format:\n",
    "# {{\"rephrased_table_name\": \"new_table_name\"}}\n",
    "# And only that, nothing more is accepted.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are the denial constraints in the given dataset:* not(t1.year==t2.year and t1.sex==t2.sex and t1.raceethnicity==t2.raceethnicity)* not(t1.deaths==t2.deaths and t1.deathrate<t2.deathrate)Explanation:* The first constraint is saying that there cannot be two tuples that have the same year, sex, raceethnicity, and death rate.* The second constraint is saying that there cannot be two tuples that have the same number of deaths and different death rates.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama = OllamaClient(model_name=\"llama2\")\n",
    "\n",
    "llama.make_request(\"\"\"A denial constraint in databases is a mathematical and logical statement that indicates what cannot be present in a dataset.\n",
    "                An example of a denial constraint is:\n",
    "                not(t1.year==t2.year and t1.sex==t2.sex and t1.raceethnicity==t2.raceethnicity and t1.deaths==t2.deaths and t1.deathrate<t2.deathrate)\n",
    "                This constraint is saying that there cannot be two tuples that have all values that satisfy the following conditions:\n",
    "                - t1.year == t2.year\n",
    "                - t1.sex == t2.sex\n",
    "                - t1.raceethnicity == t2.raceethnicity\n",
    "                - t1.deaths == t2.deaths\n",
    "                - t1.deathrate < t2.deathrate\n",
    "\n",
    "Given this other dataset, find all the denial constraints. Your response should follow the format in this example and can only contain a list of DCs:\n",
    "not(t1.year==t2.year and t1.sex==t2.sex and t1.raceethnicity==t2.raceethnicity and t1.deaths==t2.deaths and t1.deathrate<t2.deathrate)\n",
    "                   \n",
    "Dataset (csv):\n",
    "Year str,LeadingCause str,Sex str,RaceEthnicity str,Deaths int,DeathRate float,AgeAdjustedDeathRate float\n",
    "2007,Diabetes Mellitus (E10-E14),M,Other Race/ Ethnicity,11,0,0\n",
    "2010,\"Diseases of Heart (I00-I09, I11, I13, I20-I51)\",F,Not Stated/Unknown,70,0,0\n",
    "2007,Cerebrovascular Disease (Stroke: I60-I69),M,Black Non-Hispanic,213,25,33\n",
    "2007,Atherosclerosis (I70),F,Other Race/ Ethnicity,0,0,0\n",
    "2014,Malignant Neoplasms (Cancer: C00-C97),F,Black Non-Hispanic,1852,176.5,148.4\n",
    "2010,Chronic Lower Respiratory Diseases (J40-J47),F,White Non-Hispanic,501,35,20.7\n",
    "2007,\"Intentional Self-Harm (Suicide: X60-X84, Y87.0)\",M,Asian and Pacific Islander,36,7.4,7.7\n",
    "2012,All Other Causes,F,Not Stated/Unknown,53,0,0\n",
    "2009,\"Diseases of Heart (I00-I09, I11, I13, I20-I51)\",F,Hispanic,1349,112.7,143.8\n",
    "                \"\"\")\n",
    "\n",
    "# llama.make_request(\"\"\"\n",
    "#     what is the best soccer player of all time?\n",
    "# \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
